{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from datetime import datetime\n",
    "\n",
    "#Local methods:\n",
    "from alexnet import AlexNet\n",
    "from data_loader import data_loader\n",
    "\n",
    "sys.path.append('../')\n",
    "from generator import Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.reset_default_graph() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In INit\n"
     ]
    }
   ],
   "source": [
    "#INIT 1\n",
    "mode = 'train'\n",
    "'''\n",
    "Model class for you to modify for brick/ball/cylinder deep learning challenge.\n",
    "mode = 'train' or 'test' - could be a useful flag if we want to use dataset augmentations. \n",
    "'''\n",
    "if mode == 'train' or mode == 'test':\n",
    "     mode = mode\n",
    "else:\n",
    "    print('Mode must be train or test.')\n",
    "print(\"In INit\")\n",
    "# Learning params\n",
    "learning_rate = 1e-3\n",
    "minibatch_size = 32\n",
    "num_iterations = 1500\n",
    "train_test_split = 0.7\n",
    "\n",
    "# Network params\n",
    "keep_rate = 0.5 #Fraction of connections that we keep when performing dropout\n",
    "\n",
    "# How many pretrained alexnet layers to we want to load up?\n",
    "# This must be a number between 1 and 8. \n",
    "num_layers_to_load = 7\n",
    "\n",
    "# How often we want to write the tf.summary data to disk and measure performance\n",
    "display_step = 5\n",
    "save_step = 100\n",
    "\n",
    "# Path for tf.summary.FileWriter and to store model checkpoints\n",
    "save_dir = \"tf_data/sample_model\"\n",
    "\n",
    "input_image_size = (227, 227)\n",
    "label_indices = {'brick': 0, 'ball': 1, 'cylinder': 2}\n",
    "labels_ordered = list( label_indices)\n",
    "num_classes = len( label_indices)\n",
    "\n",
    "channel_means = np.array([147.12697, 160.21092, 167.70029])\n",
    "\n",
    "#Go ahead and build graph on initialization:\n",
    "#build_graph()\n",
    "\n",
    "#Set to true with we create a tf session.\n",
    "initialized = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IN BUILD\n",
      "Tensor(\"Relu_1:0\", shape=(?, 4096), dtype=float32)\n"
     ]
    }
   ],
   "source": [
    "############ def build_graph(self):\n",
    "'''\n",
    "This is where you can experiment with various architectures. \n",
    "'''\n",
    "print(\"IN BUILD\")\n",
    "# TF placeholder for graph input and output\n",
    "X = tf.placeholder(tf.float32, [None, \n",
    "                                input_image_size[0], \n",
    "                                input_image_size[1], \n",
    "                                3])\n",
    "\n",
    "y = tf.placeholder(tf.float32, [None, num_classes])\n",
    "keep_prob = tf.placeholder(tf.float32)\n",
    "\n",
    "# Build Alexnet portion of graph\n",
    "AN = AlexNet(X, keep_prob, num_layers_to_load)\n",
    "\n",
    "#### ------------ Setup Your Graph Here ------------------ ####\n",
    "\n",
    "# We're taking the front of our graph from AlexNet, now let's \n",
    "# build the rest - this is where we get to be creative!\n",
    "# Grab the tensor from alexnet that we're going to build from\n",
    "# Be sure to change this if you chnage num_layers_to_load\n",
    "# alexnet_out = AN.pool5\n",
    "alexnet_out = AN.fc7\n",
    "\n",
    "#To see the dimension of the output we're getting from alexnet, we can print our tensor:\n",
    "print(alexnet_out)\n",
    "\n",
    "def fc(x, num_in, num_out, name, relu=True):\n",
    "    \"\"\"Create a fully connected layer.\"\"\"\n",
    "    with tf.variable_scope(name) as scope:\n",
    "\n",
    "        # Create tf variables for the weights and biases\n",
    "        weights = tf.get_variable('weights', shape=[num_in, num_out],\n",
    "                                  trainable=True)\n",
    "        biases = tf.get_variable('biases', [num_out], trainable=True)\n",
    "\n",
    "        # Matrix multiply weights and inputs and add bias\n",
    "        act = tf.nn.xw_plus_b(x, weights, biases, name=scope.name)\n",
    "\n",
    "    if relu:\n",
    "        # Apply ReLu non linearity\n",
    "        relu = tf.nn.relu(act)\n",
    "        return relu\n",
    "    else:\n",
    "        return act\n",
    "fc8 = fc(AN.fc7, 4096, 3, relu=False, name='fc8')\n",
    "# all_variable_names.extend(['fc8'])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Your graph should produce an estimate our our labels, yhat.\n",
    "# yhat should be of the same dimension as y.\n",
    "# self.yhat = ?\n",
    "logits = tf.layers.dense(inputs = fc8, units = 3, activation = None, name = 'fc9')\n",
    "yhat = tf.nn.softmax(logits)\n",
    "\n",
    "# Cross Entropy\n",
    "# Implmenting cross entropy \"manually\" in tensoflow can be numerically unstable, so use this method:\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# Make sure you inlude the names of all the variables you would like to train here. \n",
    "# These may include variables from this part of the graph or the alexnet portion. \n",
    "# You can give your variables whatever name you like by passing in a name into your layers\n",
    "# for example if you setup a layer like this: tf.layers.dense(.... , name = 'my_fc6')\n",
    "# be sure to add 'my_fc6' to the trainable_variable_names list here:\n",
    "trainable_variable_names = ['fc8']\n",
    "\n",
    "#### ---------------- End Graph Setup --------------------- ####"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In TRAIN\n",
      "INFO:tensorflow:Summary name fc8/weights:0/gradient is illegal; using fc8/weights_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0/gradient is illegal; using fc8/biases_0/gradient instead.\n",
      "INFO:tensorflow:Summary name fc8/weights:0 is illegal; using fc8/weights_0 instead.\n",
      "INFO:tensorflow:Summary name fc8/biases:0 is illegal; using fc8/biases_0 instead.\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-75-3a515520e2c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     70\u001b[0m            \u001b[0mtrain_test_split\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     71\u001b[0m            \u001b[0minput_image_size\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0minput_image_size\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 72\u001b[1;33m            data_path = '..\\data')\n\u001b[0m\u001b[0;32m     73\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[1;31m#Setup minibatch generators\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\fall2018\\deep_learning\\challenge\\data_loader.py\u001b[0m in \u001b[0;36mdata_loader\u001b[1;34m(label_indices, channel_means, train_test_split, input_image_size, data_path)\u001b[0m\n\u001b[0;32m     58\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mchannel_means\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 60\u001b[1;33m             \u001b[0mclass_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mim_paths\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'/'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     61\u001b[0m             \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_indices\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mclass_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "#def train(self):\n",
    "'''\n",
    "Train model.\n",
    "'''\n",
    "print(\"In TRAIN\")\n",
    "# Start by building rest of graph that we needed for training.\n",
    "\n",
    "#### ------------ Setup Cost Function Here ------------------ ####\n",
    "\n",
    "# Your job here is to compute a cost to pass into our AdamOptimizer below. \n",
    "# cost = \n",
    "\n",
    "\n",
    "cost = tf.losses.softmax_cross_entropy(y, logits)\n",
    "\n",
    "\n",
    "#### ------------ End Cost Function Setup  ------------------ ####\n",
    "\n",
    "# We only want to train some of the variables in our overall graph:\n",
    "var_list = [v for v in tf.trainable_variables() if v.name.split('/')[0] \\\n",
    "                                            in  trainable_variable_names]\n",
    "\n",
    "# Train op\n",
    "with tf.name_scope(\"train\"):\n",
    "    # Get gradients of all trainable variables\n",
    "    gradients = tf.gradients(cost, var_list)\n",
    "    gradients = list(zip(gradients, var_list))\n",
    "\n",
    "    # Create optimizer and apply gradient descent to the trainable variables\n",
    "    optimizer = tf.train.AdamOptimizer( learning_rate)\n",
    "    train_op = optimizer.apply_gradients(grads_and_vars=gradients)\n",
    "\n",
    "# Add gradients to summary\n",
    "for gradient, var in gradients:\n",
    "    tf.summary.histogram(var.name + '/gradient', gradient)\n",
    "\n",
    "# Add the variables we train to the summary\n",
    "for var in var_list:\n",
    "    tf.summary.histogram(var.name, var)\n",
    "\n",
    "# Add the loss to summary\n",
    "tf.summary.scalar('cross_entropy', cost)\n",
    "\n",
    "# Evaluation op: Accuracy of the model\n",
    "with tf.name_scope(\"accuracy\"):\n",
    "    correct_pred = tf.equal(tf.argmax( yhat, 1), tf.argmax( y, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Add the accuracy to the summary\n",
    "tf.summary.scalar('accuracy', accuracy)\n",
    "\n",
    "# Merge all summaries together\n",
    "merged_summary = tf.summary.merge_all()\n",
    "\n",
    "# Initialize the FileWriter\n",
    "train_writer = tf.summary.FileWriter(os.path.join( save_dir, 'train'))\n",
    "test_writer = tf.summary.FileWriter(os.path.join( save_dir, 'test'))\n",
    "\n",
    "# Initialize an saver for store model checkpoints\n",
    "# We'll save the pretrained alexnet weights and the new weight values that we train:\n",
    "var_list_to_save = [v for v in tf.trainable_variables() if v.name.split('/')[0] in \\\n",
    "                                 AN.all_variable_names +  trainable_variable_names]\n",
    "\n",
    "saver = tf.train.Saver(var_list = var_list_to_save)\n",
    "\n",
    "\n",
    "#Load up data from image files. \n",
    "data = data_loader(label_indices =  label_indices, \n",
    "           channel_means =  channel_means, \n",
    "           train_test_split =  train_test_split,\n",
    "           input_image_size =  input_image_size, \n",
    "           data_path = '../data')\n",
    "\n",
    "#Setup minibatch generators\n",
    "G = Generator(data.train.X, data.train.y, minibatch_size =  minibatch_size)\n",
    "GT = Generator(data.test.X, data.test.y, minibatch_size =  minibatch_size)\n",
    "\n",
    "#Launch tf session\n",
    "sess = tf.Session()\n",
    "sess.run(tf.global_variables_initializer())\n",
    "initialized = True\n",
    "\n",
    "# Add the model graph to TensorBoard\n",
    "train_writer.add_graph( sess.graph)\n",
    "\n",
    "# Load the pretrained weights into the alexnet portion of our graph\n",
    "AN.load_initial_weights( sess)\n",
    "\n",
    "print(\"{} Start training...\".format(datetime.now()))\n",
    "print(\"{} Open Tensorboard at --logdir {}\".format(datetime.now(),\n",
    "                                                   save_dir))\n",
    "\n",
    "\n",
    "#And Train\n",
    "for i in range( num_iterations):\n",
    "    G.generate()\n",
    "    # And run the training op\n",
    "    sess.run(train_op, feed_dict={ X: G.X,  y: G.y,  keep_prob:  keep_rate})\n",
    "\n",
    "\n",
    "    # Generate summary with the current batch of data and write to file\n",
    "    if i %  display_step == 0:\n",
    "        s =  sess.run(merged_summary, feed_dict={ X: G.X,  y: G.y,  keep_prob: 1.0})\n",
    "        train_writer.add_summary(s, i)\n",
    "\n",
    "        GT.generate()\n",
    "        s =  sess.run(merged_summary, feed_dict={ X: GT.X,  y: GT.y,  keep_prob: 1.0})\n",
    "        test_writer.add_summary(s, i)\n",
    "\n",
    "        train_acc =  sess.run(accuracy, feed_dict={ X: G.X,  y: G.y,  keep_prob: 1.0})\n",
    "        test_acc =  sess.run(accuracy, feed_dict={ X: GT.X,  y: GT.y,  keep_prob: 1.0})\n",
    "\n",
    "        print(i, ' iterations,', str(G.num_epochs), 'epochs, train accuracy = ', train_acc, ', test accuracy = ', test_acc)\n",
    "\n",
    "    if i %  save_step == 0 and i > 0: \n",
    "        print(\"{} Saving checkpoint of model...\".format(datetime.now()))\n",
    "\n",
    "        # save checkpoint of the model\n",
    "        checkpoint_name = os.path.join(save_dir,\n",
    "                                       'model_epoch'+str(G.num_epochs)+'.ckpt')\n",
    "        save_path = saver.save(sess, checkpoint_name)\n",
    "\n",
    "        print(\"{} Model checkpoint saved at {}\".format(datetime.now(),\n",
    "                                                       checkpoint_name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def predict(self, X, checkpoint_dir = None):\n",
    "'''\n",
    "X: Numpy array of dimension [n, 227, 227, 3].\n",
    "Note that n here may not be the same as the minibatch_size defined above.\n",
    "\n",
    "Returns:\n",
    "yhat_numpy: A numpy array of dimension [n x 3] containing the predicted \n",
    "one hot labels for each image passed in X. \n",
    "'''\n",
    "if not initialized:\n",
    "    restore_from_checkpoint(checkpoint_dir)\n",
    "\n",
    "yhat_numpy = sess.run(yhat, feed_dict = {X : X, keep_prob: 1.0})\n",
    "\n",
    "return yhat_numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#def restore_from_checkpoint(self, checkpoint_dir = None):\n",
    "'''\n",
    "Restore model from most recent checkpoint in save dir.\n",
    "'''\n",
    "\n",
    "saver = tf.train.Saver()\n",
    "sess = tf.Session()\n",
    "\n",
    "#Load latest checkpont, use savedir if we're not given a checkpoint dir:\n",
    "if checkpoint_dir is None:\n",
    "    checkpoint_name = tf.train.latest_checkpoint(save_dir)\n",
    "else:\n",
    "    checkpoint_name = tf.train.latest_checkpoint(checkpoint_dir)\n",
    "\n",
    "print(checkpoint_name)\n",
    "\n",
    "#Resore session\n",
    "saver.restore(sess, checkpoint_name)\n",
    "initialized = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In INit\n",
      "IN BUILD\n",
      "Tensor(\"pool5:0\", shape=(?, 6, 6, 256), dtype=float32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.Model at 0x1ac8c43c630>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
